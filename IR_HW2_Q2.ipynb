{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCxs36sHuS_Q",
        "outputId": "55890284-bf4e-465d-99cf-c129041d922b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer,Input, Embedding, LSTM, GRU, Conv1D, Conv2D, GlobalMaxPool1D, Dense, Dropout,Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "import tensorflow.keras as keras\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import numpy as np\n",
        "import itertools\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77c6CcOknazf",
        "outputId": "966b864c-544c-49c9-de3c-526f5eb46ea4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WuTrdXmGaeHM"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(train, test):\n",
        "    for dataset in [train, test]:\n",
        "        for i, row in dataset.iterrows():\n",
        "            if row['question1']:\n",
        "                yield gensim.utils.simple_preprocess(row['question1'])\n",
        "            if row['question2']:\n",
        "                yield gensim.utils.simple_preprocess(row['question2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gb5HE5MKazZl"
      },
      "outputs": [],
      "source": [
        "def split_string(string_data): \n",
        "    return str(string_data).lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_2_vec_model(train, test):\n",
        "  doc = list(preprocess_data(train, test))\n",
        "  model = gensim.models.Word2Vec(doc, size=300)\n",
        "  model.train(doc, total_examples=len(doc), epochs=15)\n",
        "  return model"
      ],
      "metadata": {
        "id": "-ISi6M6a7NYr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emmbed_data(word2vec_model , dataset, embedding_dim=300 , pretrained = False ):\n",
        "    vocabs = {}\n",
        "    index_of_vocab = 0\n",
        "\n",
        "    if pretrained:\n",
        "      model = gensim.downloader.load('word2vec-google-news-300')\n",
        "    else:\n",
        "      model = word2vec_model.wv\n",
        "\n",
        "    for index, row in dataset.iterrows():\n",
        "        for question in ['question1', 'question2']:\n",
        "            ids_vector = []  \n",
        "            for word in split_string(row[question]):\n",
        "                if word in set(stopwords.words('english')):\n",
        "                    continue\n",
        "                if word in vocabs:\n",
        "                  ids_vector.append(vocabs[word])\n",
        "                else:\n",
        "                  index_of_vocab += 1\n",
        "                  vocabs[word] = index_of_vocab\n",
        "                  ids_vector.append(index_of_vocab)\n",
        "            dataset.at[index, question] = ids_vector\n",
        "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)\n",
        "    embeddings[0] = 0 \n",
        "    for word, index in vocabs.items():\n",
        "        if word in model.vocab:\n",
        "            embeddings[index] = model.word_vec(word)\n",
        "    return dataset, embeddings"
      ],
      "metadata": {
        "id": "N50NSech5tvn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padding_data(df, max_seq_length):\n",
        "    X = {'left': df['question1'], 'right': df['question2']}\n",
        "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
        "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "TPaEbjP25sDl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(n_hidden,embeddings,embedding_dim,max_seq_length, is_bidirectional = False):\n",
        "  s = Sequential()\n",
        "  s.add(Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_shape=(max_seq_length,), trainable=False))\n",
        "  if is_bidirectional:\n",
        "    s.add(Bidirectional(LSTM(n_hidden)))\n",
        "  else:\n",
        "    s.add(LSTM(n_hidden))\n",
        "  l_in = Input(shape=(max_seq_length,), dtype='int32')\n",
        "  r_in = Input(shape=(max_seq_length,), dtype='int32')\n",
        "  distance =keras.layers.Dot(axes=(1), normalize=True)([s(l_in), s(r_in)])\n",
        "  model = Model(inputs=[l_in, r_in], outputs=[distance])\n",
        "  return model"
      ],
      "metadata": {
        "id": "N3VjQu3p7ryl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MggQtf4_e8-g"
      },
      "outputs": [],
      "source": [
        "def plot_results(hist):\n",
        "  plt.subplot(211)\n",
        "  plt.plot(hist.history['accuracy'])\n",
        "  plt.plot(hist.history['val_accuracy'])\n",
        "  plt.title(' Accuracy lstm +pretrained word2vec')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "  \n",
        "  plt.subplot(212)\n",
        "  plt.plot(hist.history['loss'])\n",
        "  plt.plot(hist.history['val_loss'])\n",
        "  plt.title('Loss lstm +pretrained word2vec')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ydGjB7OUjLvs"
      },
      "outputs": [],
      "source": [
        "def save_array(address , array):\n",
        "  with open(address, 'wb') as f:\n",
        "    np.save(f, array,allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5O03O7RtfNqk"
      },
      "outputs": [],
      "source": [
        "def predict_usnig_model(model,test_df,embedding_dim,max_seq_length):\n",
        "  test_df, embeddings = emmbed_data(test_df, embedding_dim=embedding_dim, empty_w2v=not use_w2v)\n",
        "  results = []\n",
        "  test_df_unique = test_df.drop_duplicates(subset = 'qid1')\n",
        "  test_df_unique = test_df_unique.reset_index()\n",
        "  for i in range(len(test_df_unique)):\n",
        "    test_data = test_df_unique.iloc[i]\n",
        "    list_of_pairs = []\n",
        "    df_pairs = train_df[[\"qid2\",\"question2\"]].drop_duplicates(subset = 'qid2')\n",
        "    df_pairs = df_pairs.reset_index()\n",
        "    df_pairs[\"qid1\"] = test_data[\"qid1\"]\n",
        "    df_pairs[\"question1\"]=\"t\"\n",
        "    for index in df_pairs.index:\n",
        "      df_pairs.at[index, \"question1\"] = test_data[\"question1\"]\n",
        "    df_pairs_2 = padding_data(df_pairs, max_seq_length)\n",
        "    prediction = model.predict([df_pairs_2['left'], df_pairs_2['right']])\n",
        "    df_pairs[\"scores\"] = prediction\n",
        "    sort = df_pairs.sort_values(by=['scores'],ascending = False)\n",
        "    res = [sort.iloc[0][\"qid1\"],np.asarray(sort.iloc[:10][\"qid2\"])]\n",
        "    results.append(res)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
      ],
      "metadata": {
        "id": "o3BxteWI9EZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_Address = '/content/drive/MyDrive/IR/HW2/train_data.csv'\n",
        "test_Address = '/content/drive/MyDrive/IR/HW2/test_data.csv'\n",
        "validation_Address = '/content/drive/MyDrive/IR/HW2/valid_data.csv'\n",
        "\n",
        "embedding_dim = 300\n",
        "max_seq_length = 60\n",
        "batch_size = 256\n",
        "n_epoch = 15\n",
        "n_hidden = 50"
      ],
      "metadata": {
        "id": "UIPGlAYK7nE3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(train_Address)\n",
        "test_df = pd.read_csv(test_Address)\n",
        "valid_df = pd.read_csv(validation_Address)"
      ],
      "metadata": {
        "id": "rd4BlWud__i1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_2_vec_model = create_word_2_vec_model(train_df, test_df)\n",
        "train_df, embeddings = emmbed_data(word_2_vec_model,train_df, embedding_dim=embedding_dim, pretrained = False)\n",
        "valid_df, embeddings_valid = emmbed_data(word_2_vec_model,valid_df, embedding_dim=embedding_dim, pretrained = False)\n",
        "\n",
        "X_train = train_df[['question1', 'question2']]\n",
        "Y_train = train_df['is_duplicate']\n",
        "\n",
        "X_validation = valid_df[['question1', 'question2']]\n",
        "Y_validation = valid_df['is_duplicate']\n",
        "\n",
        "X_train = padding_data(X_train, max_seq_length)\n",
        "X_validation = padding_data(X_validation, max_seq_length)\n",
        "\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exmgclk_7Ryb",
        "outputId": "5c87b9f4-00e2-4eac-8bcd-16b3c3718153"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w20vmHw1a3cX"
      },
      "outputs": [],
      "source": [
        "model = create_model(n_hidden,embeddings,embedding_dim,max_seq_length, is_bidirectional = False)\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "hist = model.fit([X_train['left'], X_train['right']], Y_train,batch_size=batch_size, epochs=n_epoch,validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(hist)"
      ],
      "metadata": {
        "id": "-mEwbvgU7IDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_predict = pd.read_csv(\"/content/drive/MyDrive/IR/HW2/train_data.csv\")\n",
        "test_df_predict = pd.read_csv(\"/content/drive/MyDrive/IR/HW2/test_data.csv\")\n",
        "\n",
        "train_df_predict, _ = emmbed_data(train_df_predict, embedding_dim=embedding_dim, empty_w2v=not use_w2v)\n",
        "test_df_predict, _ = emmbed_data(test_df_predict, embedding_dim=embedding_dim, empty_w2v=not use_w2v)\n",
        "\n",
        "X_train_predict = padding_data(train_df_predict, max_seq_length)\n",
        "X_test_predict = padding_data(test_df_predict, max_seq_length)"
      ],
      "metadata": {
        "id": "s4wu5p1XiIch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_final = train_df_predict[[\"qid2\"]]\n",
        "train_dataset_final[\"tensor_2\"]=\"t\"\n",
        "tensor2 = x(X_train_predict['right']) \n",
        "for index in range(len(train_dataset_final)):\n",
        "  train_dataset_final.at[index, \"tensor_2\"] = tensor2[index].numpy()\n",
        "\n",
        "test_dataset_final = test_df_predict[[\"qid1\"]]\n",
        "test_dataset_final[\"tensor_1\"]=\"t\"\n",
        "tensor1 = x(X_test_predict['left']) \n",
        "for index in range(len(test_dataset_final)):\n",
        "  test_dataset_final.at[index, \"tensor_1\"] = tensor1[index].numpy()\n",
        "\n",
        "train_dataset_final = train_dataset_final.drop_duplicates(subset = 'qid2').reset_index()\n",
        "test_dataset_final = test_dataset_final.drop_duplicates(subset = 'qid1').reset_index()"
      ],
      "metadata": {
        "id": "yK66g6KQn1Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(len(test_dataset_final)):\n",
        "  test_data = test_dataset_final.iloc[i]\n",
        "  df_pairs = train_dataset_final.copy()\n",
        "  df_pairs[\"qid1\"] = test_data[\"qid1\"]\n",
        "  df_pairs[\"tensor_1\"]=\"t\"\n",
        "  df_pairs[\"prediction\"] = \"6\"\n",
        "  for index in df_pairs.index:\n",
        "    tens1 = test_data[\"tensor_1\"]\n",
        "    tens2 = df_pairs.iloc[index][\"tensor_2\"]\n",
        "    df_pairs.at[index, \"tensor_1\"] = tens1\n",
        "    dd = np.dot(tens1,tens2)/(norm(tens1)*norm(tens2))\n",
        "    df_pairs.at[index, \"prediction\"] = dd\n",
        "  sort = df_pairs.sort_values(by=['prediction'],ascending = False)\n",
        "  res = [sort.iloc[0][\"qid1\"],np.asarray(sort.iloc[:10][\"qid2\"])]\n",
        "  results.append(res)"
      ],
      "metadata": {
        "id": "9sZRnalxg1-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JVXBf_BjNVz"
      },
      "outputs": [],
      "source": [
        "results_save_address = '/content/drive/MyDrive/IR/HW2/results-semance-lstm-cosine.npy'\n",
        "save_array(results_save_address , results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict using model predict\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/IR/HW2/test_data.csv\")\n",
        "results = predict_usnig_model(model,test_df,300,60)"
      ],
      "metadata": {
        "id": "ha5X717z6laH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}